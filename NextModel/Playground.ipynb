{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n",
      "d:\\a__a\\projects\\szum\\venv\\lib\\site-packages\\librosa\\util\\decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "d:\\a__a\\projects\\szum\\venv\\lib\\site-packages\\librosa\\util\\decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "d:\\a__a\\projects\\szum\\venv\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import keras\n",
    "import librosa\n",
    "import python_speech_features\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from pathlib import Path\n",
    "import IPython.display as ipd\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def get_audio(path, sample_rate=16000):\n",
    "    wav, wav_sr = librosa.load(path, sr=sample_rate)\n",
    "    if wav.size < 16000:\n",
    "        return np.pad(wav, (16000 - wav.size, 0), mode='constant')\n",
    "    else:\n",
    "        return wav[0:16000]\n",
    "\n",
    "# compute MFCC features from audio signal\n",
    "def audio2feature(audio):\n",
    "    audio = audio.astype(np.float)\n",
    "    # normalize data\n",
    "    audio -= audio.mean()\n",
    "    audio /= np.max((audio.max(), -audio.min()))\n",
    "    # compute MFCC coefficients\n",
    "    features = python_speech_features.mfcc(audio, samplerate=16000, winlen=0.025, winstep=0.01, numcep=20, nfilt=40,\n",
    "                                           nfft=512, lowfreq=100, highfreq=None, preemph=0.97, ceplifter=22,\n",
    "                                           appendEnergy=True, winfunc=np.hamming)\n",
    "    return features\n",
    "\n",
    "def extract_loudest_section(audio, length):\n",
    "    audio = audio[:, 0].astype(np.float) # to avoid integer overflow when squaring\n",
    "    audio_pw = audio**2 # power\n",
    "    window = np.ones((length, ))\n",
    "    conv = np.convolve(audio_pw, window, mode=\"valid\")\n",
    "    begin_index = conv.argmax()\n",
    "    return audio[begin_index:begin_index+length]\n",
    "\n",
    "\n",
    "def signaltonoise(command, noise):\n",
    "    c = np.mean(command)\n",
    "    n = np.mean(noise)\n",
    "\n",
    "    return np.abs(c / n)\n",
    "\n",
    "\n",
    "def get_nr_noises(path, samples_nr):\n",
    "    ''' Returns paths to noise samples.'''\n",
    "    datadir = Path(path)\n",
    "    files = [str(f) for f in datadir.glob('**/*.wav') if f]\n",
    "    return files[0:samples_nr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wav = get_audio('data_new/training/bed/0c2ca723_nohash_1.wav')\n",
    "bg = get_audio('noise/a001_80_90.wav')\n",
    "start_ = np.random.randint(bg.shape[0]-16000)\n",
    "bg_slice = bg[start_ : start_+16000]\n",
    "wav_with_bg = wav * np.random.uniform(0.8, 1.2) + \\\n",
    "              bg * np.random.uniform(0, 1)\n",
    "\n",
    "librosa.output.write_wav(path=\"output2.wav\", y=wav_with_bg, sr=16000 )\n",
    " \n",
    "\n",
    "from playsound import playsound\n",
    "\n",
    "playsound(\"output2.wav\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tensorflow import keras\n",
    "\n",
    "audio_buffer = np.zeros((5, 3200))\n",
    "last_recognized_word = None\n",
    "last_recognition_time = 0\n",
    "recognition_timeout = 1.0\n",
    "word2index = {\n",
    "    # core words\n",
    "    \"yes\": 0,\n",
    "    \"no\": 1,\n",
    "    \"up\": 2,\n",
    "    \"down\": 3,\n",
    "    \"left\": 4,\n",
    "    \"right\": 5,\n",
    "    \"on\": 6,\n",
    "    \"off\": 7,\n",
    "    \"stop\": 8,\n",
    "    \"go\": 9,\n",
    "    \"unknown\": 10\n",
    "}\n",
    "index2word = [word for word in word2index]\n",
    "model = keras.models.load_model(\"models/model_January_16/model.11.hdf5\")\n",
    "\n",
    "def audio_stream_callback(indata, frames, time, status):\n",
    "    global audio_buffer\n",
    "    global model\n",
    "    global index2word\n",
    "    global last_recognized_word\n",
    "    global last_recognition_time\n",
    "    audio_buffer = np.roll(audio_buffer, shift=-1, axis=0)\n",
    "    audio_buffer[-1, :] = np.squeeze(indata)\n",
    "    recorded_feature = audio2feature(audio_buffer.flatten())\n",
    "    recorded_feature = np.expand_dims(recorded_feature, 0) # add \"fake\" batch dimension 1\n",
    "    prediction = model.predict(recorded_feature).reshape((11, ))\n",
    "    # normalize prediction output to get \"probabilities\"\n",
    "    prediction /= prediction.sum()\n",
    "    best_candidate_index = prediction.argmax()\n",
    "    best_candidate_probability = prediction[best_candidate_index]\n",
    "    \n",
    "    if(best_candidate_probability > 0.5): # treshold\n",
    "        word = index2word[best_candidate_index]\n",
    "        if( (timer()-last_recognition_time)>recognition_timeout or word!=last_recognized_word ):\n",
    "            last_recognition_time = timer()\n",
    "            last_recognized_word = word\n",
    "            clear_output(wait=True) # clear ouput as soon as new output is available to replace it\n",
    "            print(\"%s\\t:\\t%2.1f%%\" % (word, best_candidate_probability*100))\n",
    "            print(\"-----------------------------\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "unknown\t:\t78.4%\n",
      "-----------------------------\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "# REALTIME KEYWORD RECOGNITION DEMO (60s long)\n",
    "with sd.InputStream(samplerate=16000, blocksize=3200, device=None, channels=1, dtype=\"float32\", callback=audio_stream_callback):\n",
    "    sd.sleep(60*1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}